{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Techniques\n",
        "\n",
        "                                 SUBMITTED BY: MD FAHAM NAUSHAD"
      ],
      "metadata": {
        "id": "iWJqdKlcNzmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***************************************************\n",
        "##Theoretical Questions:\n",
        "\n",
        "#***************************************************\n",
        "##1. What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Anaswer:\n",
        "\n",
        "  Ensemble learning is a machine learning technique where multiple models (called base learners) are combined to produce a stronger and more accurate prediction. The key idea is that multiple weak learners can compensate for each other's errors, resulting in higher accuracy and better generalization. Ensembles reduce overfitting and improve robustness compared to single models.\n",
        "\n",
        "##2. What is the difference between Bagging and Boosting?\n",
        "- Anaswer:\n",
        "\n",
        "  Bagging trains multiple models independently on different bootstrapped samples and averages their predictions to reduce variance. Boosting trains models sequentially, where each new model focuses on correcting the errors of the previous one to reduce bias. Bagging prevents overfitting, while boosting improves weak learners.\n",
        "\n",
        "##3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Anaswer:\n",
        "\n",
        "  Bootstrap sampling is a technique where random samples are drawn with replacement from a dataset to form multiple training subsets. In Bagging models like Random Forest, each tree is trained on a different bootstrapped dataset. This increases diversity among trees, helping the ensemble reduce overfitting and improve accuracy.\n",
        "\n",
        "##4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "- Anaswer:\n",
        "\n",
        "  OOB samples are the data points that are not selected in a bootstrapped training subset. Since each tree is trained on its own bootstrap sample, OOB samples can be used as a built-in validation set. The OOB score measures how well the model predicts unseen OOB data and serves as a reliable evaluation metric without needing separate train-test splitting.\n",
        "\n",
        "##5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- Anaswer:\n",
        "\n",
        "  A single Decision Tree calculates importance based on how much each feature decreases impurity at its splits. However, its results can be unstable because it depends heavily on the training data. A Random Forest averages feature importance over many trees, making it more stable and reliable as it reduces noise and bias from individual trees."
      ],
      "metadata": {
        "id": "JzyWxY_dO8zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***************************************************\n",
        "##Practical Questions:\n",
        "\n",
        "#***************************************************"
      ],
      "metadata": {
        "id": "GDyBFkgQKVIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Write a Python program to:\n",
        "\n",
        "###Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "\n",
        "*   Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "*   Train a Random Forest Classifier\n",
        "*   Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "\n",
        "###✅Python Code:"
      ],
      "metadata": {
        "id": "fyzpCYqhKavX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilzjE239Njpf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3e8b0c-5f16-4db4-a041-c132850803a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area : 0.1394\n",
            "worst concave points : 0.1322\n",
            "mean concave points : 0.107\n",
            "worst radius : 0.0828\n",
            "worst perimeter : 0.0808\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:5]\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "for i in indices:\n",
        "    print(data.feature_names[i], \":\", round(importances[i], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Write a Python program to:\n",
        "\n",
        "*  Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "*  Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "###✅Python Code:\n"
      ],
      "metadata": {
        "id": "ERHDyimQLU2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, pred_dt)\n",
        "\n",
        "# Bagging Classifier with Decision Tree as base model\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "pred_bag = bag.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, pred_bag)\n",
        "\n",
        "print(\"Accuracy - Decision Tree:\", round(acc_dt, 4))\n",
        "print(\"Accuracy - Bagging Classifier:\", round(acc_bag, 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjV6rKchLuQz",
        "outputId": "3a89c4f0-fc7e-4540-9c60-20e87951be26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy - Decision Tree: 0.9333\n",
            "Accuracy - Bagging Classifier: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Write a Python program to:\n",
        "\n",
        "\n",
        "*   Train a Random Forest Classifier\n",
        "*   Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "*   Print the best parameters and final accuracy\n",
        "\n",
        "\n",
        "###✅Python Code:"
      ],
      "metadata": {
        "id": "HJeT-_6MLusM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                    params, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best = grid.best_estimator_\n",
        "final_acc = accuracy_score(y_test, best.predict(X_test))\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", round(final_acc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGZB353cLxjb",
        "outputId": "843d94bd-7cd2-4eb5-b7d5-7de2ca4b7841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 150}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Write a Python program to:\n",
        "\n",
        "\n",
        "*   Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "*   Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "\n",
        "###✅Python Code:"
      ],
      "metadata": {
        "id": "0AgFDobQNGIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "pred_bag = bag.predict(X_test)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "pred_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"MSE - Bagging Regressor:\", round(mean_squared_error(y_test, pred_bag), 4))\n",
        "print(\"MSE - Random Forest Regressor:\", round(mean_squared_error(y_test, pred_rf), 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGvqluNQNVLM",
        "outputId": "6e5d2d91-de5b-4ede-87a9-77cb3f5f5f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE - Bagging Regressor: 0.2579\n",
            "MSE - Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "- • You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "\n",
        "*   Choose between Bagging or Boosting\n",
        "*   Handle overfitting\n",
        "*   Select base models\n",
        "*   Evaluate performance using cross-validation\n",
        "*   Justify how ensemble learning improves decision-making\n",
        "\n",
        "- Answer:\n",
        "\n",
        "    To build a loan default prediction model, I would choose Boosting (such as Gradient Boosting/XGBoost) because it focuses on correcting mistakes made by previous models and handles complex patterns commonly found in financial data. To reduce overfitting, I would tune hyperparameters such as tree depth, learning rate, and number of estimators, and apply early stopping. I would select Decision Trees as base learners because they can capture non-linear behaviour in customer spending patterns and demographics. The model would be evaluated using cross-validation and AUC-ROC score, which helps measure classification reliability. Ensemble learning strengthens prediction quality, reducing false approvals/declines and improving business-level decision-making by lowering financial risk.\n",
        "\n",
        "\n",
        "###✅Python Code:    "
      ],
      "metadata": {
        "id": "39C8Zbp8NYlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Sample dummy dataset (simulating bank customer data)\n",
        "np.random.seed(42)\n",
        "data = pd.DataFrame({\n",
        "    'age': np.random.randint(21, 65, 500),\n",
        "    'income': np.random.randint(20000, 150000, 500),\n",
        "    'credit_score': np.random.randint(300, 850, 500),\n",
        "    'transactions_per_month': np.random.randint(10, 120, 500),\n",
        "    'loan_default': np.random.choice([0, 1], 500, p=[0.7, 0.3])  # Target variable\n",
        "})\n",
        "\n",
        "X = data.drop('loan_default', axis=1)\n",
        "y = data['loan_default']\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Gradient Boosting Classifier (Boosting approach)\n",
        "model = GradientBoostingClassifier(\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=150,\n",
        "    max_depth=3\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Cross-validation scoring\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "\n",
        "# Predict probability and compute ROC-AUC\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"Cross-Validation AUC Scores:\", np.round(cv_scores, 4))\n",
        "print(\"Mean CV AUC:\", round(cv_scores.mean(), 4))\n",
        "print(\"Test ROC-AUC Score:\", round(test_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCNJtKdlPQ-5",
        "outputId": "6fdaf877-32db-430b-b08b-12669f82d6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation AUC Scores: [0.4823 0.4523 0.43   0.5645 0.4797]\n",
            "Mean CV AUC: 0.4818\n",
            "Test ROC-AUC Score: 0.4437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###************** END  **************"
      ],
      "metadata": {
        "id": "74lX2k1WOFhu"
      }
    }
  ]
}